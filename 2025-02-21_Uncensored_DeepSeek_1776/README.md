# Uncensored DeepSeek

> **Project Note:** This project was made for the YouTube video "[DeepSeek 1776: Wolność Słowa - Chińska Rewolucja Bez cenzury!](https://www.youtube.com/watch?v=6Dpq0ctjM1s)".

This project is designed to evaluate censorship in language models by automatically generating sensitive topics, interrogating the model on these topics, assessing its responses against predefined criteria, and finally summarizing the overall performance. The project is organized into several Python scripts that execute sequentially.


## ⚠️ Required Environment Variables

- **`OPENAI_API_KEY`**: Your API key for OpenAI.
- **`OPENAI_BASE_URL`**: *(Optional)* The base URL for the OpenAI API.
- **`OPENAI_MODEL`**: The identifier of the model to be queried.

**Ensure these variables are defined in your shell or through an `.env` file before running the scripts.**

> ### Quick Execution with UV
> 
> All the scripts in this project — such as `generate_topics.py`, `interrogate_model.py`, `assess_model.py`, and `summarize.py` — can be run directly using [UV](https://docs.astral.sh/uv/). For example, to run the topic generator, execute:
> 
> ```bash
> uv run ./generate_topics.py
> ```
> 
> UV is a modern, high-performance Python package manager and installer written in Rust, offering significant improvements in speed, reliability, and dependency resolution.

### Execution with pip
All scripts include the shebang `#!/usr/bin/env python3` so they can be executed directly. Dependencies are listed in the `requirements.txt` file.

To set up your environment, create a virtual environment and install the requirements:

```bash
python3 -m venv venv
pip install -r requirements.txt
```

## Script Overview

1. **generate_topics.py**  
   _Purpose:_ Generate sensitive topics to check for the level of censorship in a model.  
   _Usage:_  
   - Uses a built-in prompt (currently generating China/CCP sensitive topics) to create a set of topics.
   - Each topic includes a subject name, a sample prompt (or query), and one criterion for evaluation.
   - **Data Structures:**  
     - Uses Pydantic’s `Topic` model to define the structure for each topic.
     - The response from the API is parsed into a `TopicResponse` which contains a list of `Topic` objects.
     - The output is a CSV file (`topics.csv`) containing fields: `subject_name`, `sample_prompt`, and `criterion`.

2. **interrogate_model.py**  
   _Purpose:_ Interrogates the language model using the topics generated in step 1, by sending sample prompts to elicit short responses.  
   _Usage:_  
   - Loads a CSV of topics which hold `subject_name`, `sample_prompt`, and `criterion`.
   - For each topic, builds a short prompt (e.g., "Please answer shortly: <sample_prompt>") and streams the model response.
   - **Data Structures:**  
     - The input CSV carries one topic per row.
     - The script outputs a CSV (`interrogate_{sanitized_model}.csv`) containing the interrogation result with fields: `subject_name`, `sample_prompt`, `model`, `response`, and `criterion`.

3. **assess_model.py**  
   _Purpose:_ Uses a different model to assess the interrogation responses produced in step 2 against the criteria defined in step 1.  
   _Usage:_  
   - Reads the interrogation CSV as input.
   - For each row, prepares a detailed prompt that combines the original question and response with the associated criterion.
   - Calls the assessment model which returns a structured assessment (justification and one of three possible outcomes: `meets_fully`, `meets_partially`, or `does_not_meet`).
   - **Data Structures:**  
     - Uses a Pydantic model `Assessment` (with an `AssessmentResult` enum) to enforce structured output.
     - Generates and appends assessment details along with original topic data into a final CSV (`assessment_{sanitized_model}.csv`).

4. **summarize.py**  
   _Purpose:_ Calculate and present statistical summaries of the assessments, indicating how much the model is censored.  
   _Usage:_  
   - Reads the assessment CSV generated by **assess_model.py**.
   - Groups the results based on the assessment outcome.
   - Displays statistics (counts and percentages) for each outcome group.
   - Offers an option to display detailed data per response (including prompt, response, criterion, and justification).
   - **Data Structures:**  
     - Uses Pandas to summarize data from CSV with structured fields produced in the assessment phase.

  ## Architecture Overview

  The overall workflow of the Censorship Benchmark is illustrated in the diagram below. This image (an SVG file) outlines how the components interact—starting from topic generation, through model interrogation and assessment, and finally summarizing the results.

  ![Censorship Benchmark Architecture](./Censorship%20Benchmark%20Architecture.drawio.svg)
