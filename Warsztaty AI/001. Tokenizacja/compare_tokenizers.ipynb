{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import eventlet\n",
    "\n",
    "book_files = {\n",
    "    \"Mickiewicz\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/pan-tadeusz.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dziady-dziady-widowisko-czesc-i.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dziady-dziadow-czesci-iii-ustep-do-przyjaciol-moskali.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/ballady-i-romanse-pani-twardowska.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/ballady-i-romanse-powrot-taty.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/ballady-i-romanse-switez.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dziady-dziady-poema-dziady-czesc-iv.txt\",\n",
    "    ],\n",
    "    \"Sienkiewicz\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/quo-vadis.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/sienkiewicz-we-mgle.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/potop-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/potop-tom-drugi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/potop-tom-trzeci.txt\",\n",
    "    ],\n",
    "    \"Orzeszkowa\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/orzeszkowa-kto-winien.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/nad-niemnem-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/nad-niemnem-tom-drugi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/nad-niemnem-tom-trzeci.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/gloria-victis-dziwna-historia.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/z-pozogi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/pani-dudkowa.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dymy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/syn-stolarza.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/dobra-pani.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/cnotliwi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/kilka-slow-o-kobietach.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/patryotyzm-i-kosmopolityzm.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/julianka.txt\",\n",
    "    ],\n",
    "    \"Prus\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/lalka-tom-drugi.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/lalka-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/antek.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/katarynka.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/prus-anielka.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/prus-placowka.txt\",\n",
    "    ],\n",
    "    \"Reymont\": [\n",
    "        \"https://wolnelektury.pl/media/book/txt/ziemia-obiecana-tom-pierwszy.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/chlopi-czesc-pierwsza-jesien.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/reymont-chlopi-zima.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/chlopi-czesc-trzecia-wiosna.txt\",\n",
    "        \"https://wolnelektury.pl/media/book/txt/chlopi-czesc-czwarta-lato.txt\",\n",
    "    ],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!mkdir -p data",
   "id": "e76ac827054db96a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from urllib import request\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch(url):\n",
    "    \"\"\"Download a file and save it to the data directory.\"\"\"\n",
    "    file_path = os.path.join(\"data\", os.path.basename(url))\n",
    "    if os.path.exists(file_path):\n",
    "        return None, None\n",
    "    data = request.urlopen(url).read()\n",
    "    return file_path, data\n",
    "\n",
    "\n",
    "def download():\n",
    "    \"\"\"Download all books from the book_files dictionary.\"\"\"\n",
    "    for author in book_files:\n",
    "        pool = eventlet.GreenPool()\n",
    "\n",
    "        for file_path, data in pool.imap(fetch, book_files[author]):\n",
    "            if file_path:\n",
    "                with open(file_path, mode=\"wb\") as f:\n",
    "                    f.write(data)\n",
    "\n",
    "download()\n",
    "print(\"DONE\")"
   ],
   "id": "ad02b8dde8d8437b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!ls -la data",
   "id": "b6191ac6b97b82f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c52d6790d12188f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizers = [\n",
    "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"microsoft/phi-4\",\n",
    "    \"deepseek-ai/DeepSeek-R1\",\n",
    "]"
   ],
   "id": "9ed267b9ca35028e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def create_corpus_and_stats(data_dir=\"./data\"):\n",
    "    \"\"\"Create a corpus from text files in the data directory and compute basic statistics.\"\"\"\n",
    "    corpus = \" \".join(file.read_text(encoding=\"utf-8\") for file in Path(data_dir).glob(\"*.txt\"))\n",
    "    # Normalize whitespace quickly with split/join (faster than regex on huge texts; see https://stackoverflow.com/q/2077897)\n",
    "    corpus = \" \".join(corpus.split())\n",
    "    num_chars = len(corpus)\n",
    "    num_words = corpus.count(\" \") + 1 if corpus else 0\n",
    "    return corpus, {\"num_chars\": num_chars, \"num_words\": num_words}\n",
    "\n",
    "def tokenize_and_compute_stats(corpus, corpus_stats, tokenizer):\n",
    "    \"\"\"Tokenize the corpus and compute statistics.\"\"\"\n",
    "    # If corpus is within max_length, tokenize directly.\n",
    "    max_length = tokenizer.model_max_length\n",
    "    if len(corpus) <= max_length:\n",
    "        tokens = tokenizer(corpus, add_special_tokens=False)[\"input_ids\"]\n",
    "    else:\n",
    "        tokens = []\n",
    "        start, corpus_len = 0, len(corpus)\n",
    "        while start < corpus_len:\n",
    "            end = start + max_length\n",
    "            if end >= corpus_len:\n",
    "                chunk = corpus[start:]\n",
    "                start = corpus_len\n",
    "            else:\n",
    "                # Find last whitespace in the window [start, end)\n",
    "                split_index = corpus.rfind(\" \", start, end)\n",
    "                if split_index <= start:  # No whitespace found; force split at max_length\n",
    "                    split_index = end\n",
    "                chunk = corpus[start:split_index]\n",
    "                start = split_index  # Leave the whitespace in the remainder\n",
    "            if chunk:\n",
    "                tokens.extend(tokenizer(chunk, add_special_tokens=False)[\"input_ids\"])\n",
    "            else:\n",
    "                start += 1  # Avoid stalling on empty chunks\n",
    "    num_tokens = len(tokens)\n",
    "    num_chars = corpus_stats[\"num_chars\"]\n",
    "    num_words = corpus_stats[\"num_words\"]\n",
    "    stats = {\n",
    "        \"tokenizer_name\": tokenizer.name_or_path,\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"avg_tokens_per_word\": num_tokens / num_words if num_words else 0,\n",
    "        \"avg_chars_per_token\": num_chars / num_tokens if num_tokens else 0,\n",
    "        \"avg_words_per_token\": num_words / num_tokens if num_tokens else 0,\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "    }\n",
    "    return tokens, stats"
   ],
   "id": "6f32d3ebc853eaf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "corpus, corpus_stats = create_corpus_and_stats()",
   "id": "7a803913262c1edc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "corpus_stats",
   "id": "2534bb2c97b89901",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def tokenize_stats(tokenizer_name, corpus=corpus, corpus_stats=corpus_stats):\n",
    "    \"\"\"Tokenize the corpus and compute statistics for a given tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokens, stats = tokenize_and_compute_stats(corpus, corpus_stats, tokenizer)\n",
    "    print(f\"Tokenizer: {tokenizer_name}\")\n",
    "    # print(f\"Tokens: {stats['num_tokens']}\")\n",
    "    print(f\"Average tokens per word: {stats['avg_tokens_per_word']:.2f}\")\n",
    "    print(f\"Average characters per token: {stats['avg_chars_per_token']:.2f}\")\n",
    "    print(f\"Average words per token: {1 / stats['avg_tokens_per_word']:.2f}\")\n",
    "    print(\"\\n\")\n",
    "    return stats"
   ],
   "id": "d62a8950a89a500b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "stats_df = pd.DataFrame([tokenize_stats(tokenizer) for tokenizer in tokenizers])\n",
    "stats_df"
   ],
   "id": "3058158445516f4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "809424b56a1c9b26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e7c3cb9ce621c6fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "936bdb7f15b1eb9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a86dce066fe8ba19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
