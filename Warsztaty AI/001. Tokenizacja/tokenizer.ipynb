{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer:PreTrainedTokenizerFast = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "1d0b9f49f8752eeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(tokenizer.vocab.keys())",
   "id": "be2b64232addf195",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(list(range(128256)))\n",
    "tokens[:24]"
   ],
   "id": "bc910f3fa4936123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokens[-256:-240]",
   "id": "4d5660d86634fae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=False)",
   "id": "64ed923f0667b25f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.encode(\" \", add_special_tokens=False)",
   "id": "4946ee553c1695b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokens[9906]",
   "id": "c6c9c464a5ddeab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer.tokenize(\"Hello, my dog is cute\")",
   "id": "4b27d12ebcab203",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def fix_token(token: str):\n",
    "    \"\"\"Fix token for display.\"\"\"\n",
    "    if token.startswith('Ġ'):\n",
    "        # Count number of Ġ characters\n",
    "        space_count = token.count('Ġ')\n",
    "        # Replace with middle dot (·) for each space\n",
    "        return '·' * space_count + token[space_count:]\n",
    "\n",
    "    return token\n",
    "\n",
    "def visualize_tokenization(text: str, tokenizer=tokenizer, monospace=False):\n",
    "    \"\"\"Visualize tokenization of a text.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    palette = [\"#FFB6C1\", \"#87CEFA\", \"#98FB98\", \"#FFDAB9\", \"#E6E6FA\", \"#FFDEAD\", \"#FFE4B5\"]\n",
    "\n",
    "    # Set font family based on monospace parameter\n",
    "    font_family = \"'Courier New', Courier, monospace\" if monospace else \"inherit\"\n",
    "\n",
    "    token_spans = []\n",
    "    line_break = False\n",
    "    for i, token in enumerate(tokens):\n",
    "        fixed_token = fix_token(token)\n",
    "\n",
    "        # Skip wrapping newline tokens in colored spans\n",
    "        if fixed_token.endswith('Ċ'):\n",
    "            fixed_token = fixed_token[:-1]\n",
    "            line_break = True\n",
    "\n",
    "        token_spans.append(\n",
    "            f'<span style=\"background-color: {palette[i % len(palette)]}; '\n",
    "            f'color: black; padding: 2px 1px; border-radius: 0px; '\n",
    "            f'display: inline-block; font-family: {font_family};\">'\n",
    "            f'{fixed_token}</span>'\n",
    "        )\n",
    "        if line_break:\n",
    "            token_spans.append('<br>')\n",
    "            line_break = False\n",
    "\n",
    "    html_content = \"\".join(token_spans)\n",
    "    display(HTML(html_content))"
   ],
   "id": "4e10202bfb45ee79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_tokenization(\"Hello, my dog is cute\")",
   "id": "2fec6d0f1596e7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_tokenization(\"The tokenization process is fun. Superfun\")",
   "id": "1ffa61d29e73a2fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "visualize_tokenization(\"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit,\n",
    "sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
    "Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
    "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
    "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\"\", monospace=True)"
   ],
   "id": "331b95df22ee1b53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "35124d29c409fb49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "visualize_tokenization('''def fetch(url):\n",
    "    \"\"\"Download a file and save it to the data directory.\"\"\"\n",
    "    file_path = os.path.join(\"data\", os.path.basename(url))\n",
    "    if os.path.exists(file_path):\n",
    "        return None, None\n",
    "    data = request.urlopen(url).read()\n",
    "    return file_path, data\n",
    "''', monospace=True)"
   ],
   "id": "bf2cab4c69adbc22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "visualize_tokenization('''#include <stdio.h>\n",
    "int main(void)\n",
    "{\n",
    "\tprintf(\"Hello World!\\n\");\n",
    "    return 0;\n",
    "}''', monospace=True)"
   ],
   "id": "a40894301fe36602",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9d62d62a03cf58d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "deepseek_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")"
   ],
   "id": "5d08c4e81a05c254",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(deepseek_tokenizer.vocab.keys())",
   "id": "dcc4476ed5d4aaf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "deepseek_tokenizer.convert_ids_to_tokens(list(range(128815-20, 128815)))",
   "id": "378ea885bdb6659c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "190dc9f92ccd2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "phi4_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-4\")\n",
    "len(phi4_tokenizer.vocab.keys())"
   ],
   "id": "e15f9cad80fe4f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1170ba66812c4501",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eebfa15a186d632f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "364e60d13b88e41d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model: LlamaForCausalLM = pipeline.model\n",
    "\n",
    "# Access the embedding layer\n",
    "embedding_layer = model.get_input_embeddings()"
   ],
   "id": "edcfa33d797081e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the full embedding matrix\n",
    "embedding_matrix = embedding_layer.weight.detach().cpu().numpy()\n",
    "\n",
    "embedding_matrix.shape"
   ],
   "id": "dff253775c25f52a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "85699e58e1be53d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(metric=\"cosine\").fit(embedding_matrix)"
   ],
   "id": "4c5535034d580677",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_closest(token: str, n_neighbors: int = 6, threshold: float = 0.5):\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    distance, indices = nn.kneighbors(embedding_matrix[token_id].reshape(1, -1), n_neighbors=n_neighbors)\n",
    "    closest_tokens =  indices[0]\n",
    "    # remove tokens farther than .5 cosine distance\n",
    "    closest_tokens = [i for i, d in zip(closest_tokens, distance[0]) if d < threshold]\n",
    "    # remove the original token and convert the ids to tokens\n",
    "    return [tokenizer.convert_ids_to_tokens([i])[0] for i in closest_tokens if i != token_id]"
   ],
   "id": "5a8ab2bd8adb8823",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f15aa3b0a2650137",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "find_closest(\"Ġwoman\")",
   "id": "8d5a127045c1bc40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eac755c62e0304d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9db22a2ec4131972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "married_embedding, woman_embedding, male_embedding = embedding_matrix[tokenizer.convert_tokens_to_ids([\"Ġmarried\", \"Ġwoman\", \"Ġdoor\"])]",
   "id": "2b4c7ff733ed37bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3b12a819ab2dd169",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def find_closest_by_embedding(embedding, n_neighbors=6):\n",
    "    _, indices = nn.kneighbors(embedding.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "    closest_tokens =  indices[0]\n",
    "    return [tokenizer.convert_ids_to_tokens([i])[0] for i in closest_tokens]"
   ],
   "id": "dddc7bc6aa2effb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "find_closest_by_embedding(married_embedding)",
   "id": "cd74e9d2483fae86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4907c09e3ea36699",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "difference = woman_embedding + married_embedding\n",
    "# difference = difference / np.linalg.norm(difference)"
   ],
   "id": "9e669a295f25355",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "king, man, woman = embedding_matrix[tokenizer.convert_tokens_to_ids([\"Ġking\", \"Ġman\", \"Ġwoman\"])]",
   "id": "76c16de4ac808cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f0a5100c324f2aba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def spherical_subtraction(minuend, subtrahend, base=None, eps=1e-10):\n",
    "    # Compute the dot product and clip to avoid numerical issues ([NumPy.clip](https://numpy.org/doc/stable/reference/generated/numpy.clip.html))\n",
    "    dot = np.dot(minuend, subtrahend)\n",
    "    dot = np.clip(dot, -1.0, 1.0)\n",
    "    theta = np.arccos(dot)\n",
    "\n",
    "    # Compute the spherical logarithm map (geodesic difference) from minuend to subtrahend\n",
    "    if theta < eps:\n",
    "        diff = np.zeros_like(minuend)\n",
    "    else:\n",
    "        diff = (theta / np.sin(theta)) * (subtrahend - dot * minuend)\n",
    "\n",
    "    if base is None:\n",
    "        return diff\n",
    "    else:\n",
    "        norm_diff = np.linalg.norm(diff)\n",
    "        if norm_diff < eps:\n",
    "            return base\n",
    "        # Apply the spherical exponential map to rotate the base vector ([Exponential map (Riemannian geometry)](https://en.wikipedia.org/wiki/Exponential_map_(Riemannian_geometry)))\n",
    "        return np.cos(norm_diff)*base + np.sin(norm_diff)*(diff/norm_diff)\n"
   ],
   "id": "9d7fc261e7702cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "find_closest_by_embedding(spherical_subtraction(king, man, woman))",
   "id": "d3950d3abdad2156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b421abad1d250321",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
